# The Transformer Cathedral

Attention mechanisms and self-attention layers. Where queries, keys, and values dance to create understanding. The architecture powering modern LLMs.
